\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\citation{Wang2015Detection}
\citation{Yongle2015Zero}
\citation{Hinton2012A}
\citation{vapnik1999overview}
\citation{Kramer1990Diagnosis}
\citation{guangquan2016fault}
\citation{jia2016deep}
\citation{YiThe}
\HyPL@Entry{0<</S/D>>}
\newcplabel{^_1}{1}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Wavelet Packet Decomposition and Feature Extraction}{1}{section.2}}
\citation{Walczak1997Noise}
\citation{ZHANG201914}
\citation{hinton2006fast}
\citation{hinton1986learning}
\citation{zhangchunxia}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Difference between wavelet decomposition and wavelet packet decomposition.}}{2}{figure.1}}
\newlabel{fig:wpd}{{1}{2}{Difference between wavelet decomposition and wavelet packet decomposition}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Deep Belief Network}{2}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {{3.1}}Restricted Boltzmann Machine}{2}{subsection.3.1}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Statistical Features}}{2}{table.1}}
\newlabel{tab:sf}{{1}{2}{Statistical Features}{table.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces A simple DBN model with two RBMs and one classifier.}}{2}{figure.2}}
\newlabel{fig:dbn}{{2}{2}{A simple DBN model with two RBMs and one classifier}{figure.2}{}}
\citation{zhangchunxia}
\citation{Hinton2002Training}
\citation{YiThe}
\citation{wang2017improved}
\citation{YiThe}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces A simple RBM model with three hidden layer units and five visible layer units.}}{3}{figure.3}}
\newlabel{fig:rbm}{{3}{3}{A simple RBM model with three hidden layer units and five visible layer units}{figure.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {{3.2}}Contrastive Divergence Algorithm}{3}{subsection.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {{3.3}}Train DBN}{3}{subsection.3.3}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Contrastive Divergence}}{3}{algorithm.1}}
\newlabel{alg:cd}{{1}{3}{Contrastive Divergence Algorithm}{algorithm.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Error surface with multiple grooves. The yellow pentagram indicates the local minimum point, and the red pentagram indicates the global minimum point. The yellow dot indicates the starting point when RBM is not used. The red dot indicates the starting point when RBM is used. The yellow and red curves indicate the gradient descent routes. Obviously, the red starting point is more helpful for training to get a better classification model.}}{3}{figure.4}}
\newlabel{fig:surface}{{4}{3}{Error surface with multiple grooves. The yellow pentagram indicates the local minimum point, and the red pentagram indicates the global minimum point. The yellow dot indicates the starting point when RBM is not used. The red dot indicates the starting point when RBM is used. The yellow and red curves indicate the gradient descent routes. Obviously, the red starting point is more helpful for training to get a better classification model}{figure.4}{}}
\citation{wang2017improved}
\citation{data}
\citation{guangquan2016fault}
\@writefile{toc}{\contentsline {subsection}{\numberline {{3.4}}Correction of Problems}{4}{subsection.3.4}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments}{4}{section.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The above figure is the raw data, and the figure below is the data after noise reduction.}}{4}{figure.5}}
\newlabel{fig:pre}{{5}{4}{The above figure is the raw data, and the figure below is the data after noise reduction}{figure.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The figure includes a total of 36 axes. In each axis, curves of different colors represent different fault type data.}}{4}{figure.6}}
\newlabel{fig:feature}{{6}{4}{The figure includes a total of 36 axes. In each axis, curves of different colors represent different fault type data}{figure.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {{4.1}}DBN}{4}{subsection.4.1}}
\citation{YiThe}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Comparison of classification accuracy between traditional back propagation neural network and DBN in 1000 epoches.}}{5}{figure.7}}
\newlabel{fig:dbnVSbp}{{7}{5}{Comparison of classification accuracy between traditional back propagation neural network and DBN in 1000 epoches}{figure.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {{4.2}}Isigmoid}{5}{subsection.4.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Comparison of classification accuracy between DBN using Sigmoid and DBN using Isigmoid in 1000 epoches.}}{5}{figure.8}}
\newlabel{fig:SigmoidVSIsigmoid}{{8}{5}{Comparison of classification accuracy between DBN using Sigmoid and DBN using Isigmoid in 1000 epoches}{figure.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {{4.3}}SSDBN}{5}{subsection.4.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Comparison of classification accuracy between DBN and SSDBN in 1000 epoches.}}{5}{figure.9}}
\newlabel{fig:SSDBN}{{9}{5}{Comparison of classification accuracy between DBN and SSDBN in 1000 epoches}{figure.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Comparison of reconstruction error between DBN and SSDBN.}}{5}{figure.10}}
\newlabel{fig:RBM012}{{10}{5}{Comparison of reconstruction error between DBN and SSDBN}{figure.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Comparison of classification accuracy between SSDBN using Sigmoid and SSDBN using Isigmoid in 1000 epoches.}}{5}{figure.11}}
\newlabel{fig:SSDBNIsigmoid}{{11}{5}{Comparison of classification accuracy between SSDBN using Sigmoid and SSDBN using Isigmoid in 1000 epoches}{figure.11}{}}
\bibstyle{unsrt}
\bibdata{ref}
\bibcite{Wang2015Detection}{{1}{}{{}}{{}}}
\bibcite{Yongle2015Zero}{{2}{}{{}}{{}}}
\bibcite{Hinton2012A}{{3}{}{{}}{{}}}
\bibcite{vapnik1999overview}{{4}{}{{}}{{}}}
\bibcite{Kramer1990Diagnosis}{{5}{}{{}}{{}}}
\bibcite{guangquan2016fault}{{6}{}{{}}{{}}}
\bibcite{jia2016deep}{{7}{}{{}}{{}}}
\bibcite{YiThe}{{8}{}{{}}{{}}}
\bibcite{Walczak1997Noise}{{9}{}{{}}{{}}}
\bibcite{ZHANG201914}{{10}{}{{}}{{}}}
\bibcite{hinton2006fast}{{11}{}{{}}{{}}}
\bibcite{hinton1986learning}{{12}{}{{}}{{}}}
\bibcite{zhangchunxia}{{13}{}{{}}{{}}}
\bibcite{Hinton2002Training}{{14}{}{{}}{{}}}
\bibcite{wang2017improved}{{15}{}{{}}{{}}}
\bibcite{data}{{16}{}{{}}{{}}}
\global\@namedef{@lastpage@}{6}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusions}{6}{section.5}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
